{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/keisen/tf-keras-vis/blob/master/examples/attentions.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention with Saliency and Class Activation Map\n",
    "\n",
    "This notebook explain how to get various attention images with Saliency, SmoothGrad, GradCAM, GradCAM++ and ScoreCAM/Faster-ScoreCAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "### Install libraries\n",
    "\n",
    "At first, when you've NOT yet installed `tf-keras-vis` in your environment such Google Colab, please run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade tf-keras-vis tensorflow matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tf_keras_vis.utils import num_of_gpus\n",
    "\n",
    "_, gpus = num_of_gpus()\n",
    "print('Tensorflow recognized {} GPUs'.format(gpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tf.keras.Model\n",
    "\n",
    "This example notebook uses VGG16 model in tf.keras, but if you want to use other tf.keras.Model, you can do so by modifying the section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16 as Model\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Load model\n",
    "model = Model(weights='imagenet', include_top=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images\n",
    "\n",
    "tf-keras-vis support evaluating batch-wisely that includes multiple images. Here, we load three pictures of goldfish, bear and assault-rifle as inputs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "# Image titles\n",
    "image_titles = ['Goldfish', 'Bear', 'Assault rifle']\n",
    "\n",
    "# Load images and Convert them to a Numpy array\n",
    "img1 = load_img('images/goldfish.jpg', target_size=(224, 224))\n",
    "img2 = load_img('images/bear.jpg', target_size=(224, 224))\n",
    "img3 = load_img('images/soldiers.jpg', target_size=(224, 224))\n",
    "images = np.asarray([np.array(img1), np.array(img2), np.array(img3)])\n",
    "\n",
    "# Preparing input data for VGG16\n",
    "X = preprocess_input(images)\n",
    "\n",
    "# Rendering\n",
    "subplot_args = { 'nrows': 1, 'ncols': 3, 'figsize': (12, 4),\n",
    "                 'subplot_kw': {'xticks': [], 'yticks': []} }\n",
    "f, ax = plt.subplots(**subplot_args)\n",
    "for i, title in enumerate(image_titles):\n",
    "    ax[i].set_title(title, fontsize=18)\n",
    "    ax[i].imshow(images[i])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define necessary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Modifier function\n",
    "\n",
    "First, when the softmax activation function is applied to the last layer of model, it may obstruct generating the attention images, so you need to replace the function to a linear function. Here, we will do so using `model_modifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_modifier(cloned_model):\n",
    "    cloned_model.layers[-1].activation = tf.keras.activations.linear\n",
    "    return cloned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score function\n",
    "\n",
    "And then, you MUST define `score` function that return target scores. Here, it returns the scores corresponding Goldfish, Bear, Assault Rifle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_keras_vis.utils.scores import CategoricalScore\n",
    "\n",
    "# 1 is the imagenet index corresponding to Goldfish, 294 to Bear and 413 to Assault Rifle.\n",
    "score = CategoricalScore([1, 294, 413])\n",
    "\n",
    "# Instead of CategoricalScore function object,\n",
    "# you can also define the scratch function such as below:\n",
    "def score_function(output):\n",
    "    # The `output` variable refer to the output of the model,\n",
    "    # so, in this case, `output` shape is `(3, 1000)` i.e., (samples, classes).\n",
    "    return (output[0][1], output[1][294], output[2][413])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Saliency\n",
    "\n",
    "`Saliency` generate the saliency map that appears input regions that a change in the input value contribute the output value.\n",
    "\n",
    "BTW, All attentions class constructors have `clone` option. When True, the model will be cloned, so the model instance will be NOT modified by attentions class, but it may take a while. \n",
    "In this notebook, because it does NOT need to do so, all setting False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tensorflow.keras import backend as K\n",
    "from tf_keras_vis.saliency import Saliency\n",
    "from tf_keras_vis.utils import normalize\n",
    "\n",
    "# Create Saliency object.\n",
    "saliency = Saliency(model,\n",
    "                    model_modifier=model_modifier,\n",
    "                    clone=False)\n",
    "\n",
    "# Generate saliency map\n",
    "saliency_map = saliency(score, X)\n",
    "\n",
    "# Render\n",
    "f, ax = plt.subplots(**subplot_args)\n",
    "for i, title in enumerate(image_titles):\n",
    "    ax[i].set_title(title, fontsize=14)\n",
    "    ax[i].imshow(saliency_map[i], cmap='jet')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SmoothGrad\n",
    "\n",
    "As you can see above, Vanilla Saliency map is too noisy, so let's remove noise in the saliency map using SmoothGrad! SmoothGrad is a method that reduce noise including saliency map by adding noise to input image.\n",
    "\n",
    "**Note:** Because SmoothGrad calculates gradients repeatedly, it takes much time around 2-3 minutes when using CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Generate saliency map with smoothing that reduce noise by adding noise\n",
    "saliency_map = saliency(score,\n",
    "                        X,\n",
    "                        smooth_samples=20, # The number of calculating gradients iterations.\n",
    "                        smooth_noise=0.20) # noise spread level.\n",
    "\n",
    "f, ax = plt.subplots(**subplot_args)\n",
    "for i, title in enumerate(image_titles):\n",
    "    ax[i].set_title(title, fontsize=14)\n",
    "    ax[i].imshow(saliency_map[i], cmap='jet')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/smoothgrad.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradCAM\n",
    "\n",
    "Saliency is one of useful way of visualizing attention that appears input regions that a change in the input value contribute the output value.\n",
    "GradCAM is another way of visualizing attention over input. Instead of using gradients with respect to model outputs, it uses penultimate (pre Dense layer) Conv layer output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from matplotlib import cm\n",
    "from tf_keras_vis.gradcam import Gradcam\n",
    "\n",
    "# Create Gradcam object\n",
    "gradcam = Gradcam(model,\n",
    "                  model_modifier=model_modifier,\n",
    "                  clone=False)\n",
    "\n",
    "# Generate heatmap with GradCAM\n",
    "cam = gradcam(score,\n",
    "              X,\n",
    "              penultimate_layer=-1) # model.layers number\n",
    "\n",
    "f, ax = plt.subplots(**subplot_args)\n",
    "for i, title in enumerate(image_titles):\n",
    "    heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n",
    "    ax[i].set_title(title, fontsize=14)\n",
    "    ax[i].imshow(images[i])\n",
    "    ax[i].imshow(heatmap, cmap='jet', alpha=0.5) # overlay\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, GradCAM is useful method for intuitively knowing where the attention is. But when you take a look closely, especially the head of Bear, you'll see that the visualized attentions don't completely cover the target (i.e., the Bear) that appear in the picture.\n",
    "\n",
    "Okay then, let's move on to next method that is able to improve the problem above you looked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradCAM++\n",
    "\n",
    "GradCAM++ can provide better visual explanations of CNN model predictions.\n",
    "In tf-keras-vis, GradcamPlusPlus (GradCAM++) class has most of compatibility with Gradcam. So you can use GradcamPlusPlus if you just replace classname from Gradcam to GradcamPlusPlus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tf_keras_vis.gradcam import GradcamPlusPlus\n",
    "\n",
    "# Create GradCAM++ object and repalce class name to \"GradcamPlusPlus\"\n",
    "# gradcam = Gradcam(model,\n",
    "#                   model_modifier,\n",
    "#                   clone=False)\n",
    "gradcam = GradcamPlusPlus(model,\n",
    "                          model_modifier,\n",
    "                          clone=False)\n",
    "\n",
    "# Generate heatmap with GradCAM++\n",
    "cam = gradcam(score,\n",
    "              X,\n",
    "              penultimate_layer=-1)\n",
    "\n",
    "f, ax = plt.subplots(**subplot_args)\n",
    "for i, title in enumerate(image_titles):\n",
    "    heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n",
    "    ax[i].set_title(title, fontsize=14)\n",
    "    ax[i].imshow(images[i])\n",
    "    ax[i].imshow(heatmap, cmap='jet', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/gradcam_plus_plus.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, Now, the visualized attentions almost completely cover the target objects!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ScoreCAM\n",
    "\n",
    "Lastly, Here, we show you ScoreCAM. SocreCAM is an another method that generate Class Activation Map. The characteristic of this method is that it's the `gradient-free` CAM method unlike GradCAM/GradCAM++.\n",
    "\n",
    "In default, this method takes too much time, so in the cell below ScoreCAM will be NOT run with CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tf_keras_vis.scorecam import ScoreCAM\n",
    "from tf_keras_vis.utils import num_of_gpus\n",
    "\n",
    "# This cell takes toooooooo much time, so only doing with GPU.\n",
    "_, gpus = num_of_gpus()\n",
    "if gpus > 0:\n",
    "    # Create ScoreCAM object\n",
    "    scorecam = ScoreCAM(model, model_modifier, clone=False)\n",
    "\n",
    "    # Generate heatmap with ScoreCAM\n",
    "    cam = scorecam(score,\n",
    "                   X,\n",
    "                   penultimate_layer=-1)\n",
    "\n",
    "    f, ax = plt.subplots(**subplot_args)\n",
    "    for i, title in enumerate(image_titles):\n",
    "        heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n",
    "        ax[i].set_title(title, fontsize=14)\n",
    "        ax[i].imshow(images[i])\n",
    "        ax[i].imshow(heatmap, cmap='jet', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Let's run a next cell to show Faster-ScoreCAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster-ScoreCAM\n",
    "\n",
    "As you see above, ScoreCAM need huge processing power, but there is a good news for us. Faster-ScorecAM that makes ScoreCAM to be more efficient was devised by @tabayashi0117.\n",
    "\n",
    "https://github.com/tabayashi0117/Score-CAM/blob/master/README.md#faster-score-cam\n",
    "> We thought that several channels were dominant in generating the final heat map. Faster-Score-CAM adds the processing of “use only channels with large variances as mask images” to Score-CAM. (max_N = -1 is the original Score-CAM).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tf_keras_vis.scorecam import ScoreCAM\n",
    "\n",
    "# Create ScoreCAM object\n",
    "scorecam = ScoreCAM(model, model_modifier, clone=False)\n",
    "\n",
    "# Generate heatmap with Faster-ScoreCAM\n",
    "cam = scorecam(score,\n",
    "               X,\n",
    "               penultimate_layer=-1,\n",
    "               max_N=10)\n",
    "\n",
    "f, ax = plt.subplots(**subplot_args)\n",
    "for i, title in enumerate(image_titles):\n",
    "    heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n",
    "    ax[i].set_title(title, fontsize=14)\n",
    "    ax[i].imshow(images[i])\n",
    "    ax[i].imshow(heatmap, cmap='jet', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it's still slow, so we recommend to use GradCAM++ if you don't have any reason such as you need gradient-free methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
